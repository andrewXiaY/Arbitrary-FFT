Parallel FFT using OpenMPI
==========================

## Disclaimer
* All information provided about NYU Prince cluster in this file is up to date
  as of May 6 2020.
* The results related to MPI used in the presentation was out dated. Those
  results are regenerated in this commit. The regenerated data is NOT what the
  slides shows.
* See the section [Notes on Partitions](#notes-on-partitions) for reason why
  test results are regenerated and where the old results are.

## Final Version
* `mpi-h.cpp`: The final MPI implementation of this project
  (using Bluestein's algorithm)
* `fftw-mpi.cpp`: The reference implementation using FFTW-MPI

## Build and Run on NYU Prince Cluster

### Short Version
1. `$ sh mkall.sh`: Build all tools and implementations
2. `$ cd jobs`
3. `$ sh gen.sh`: Generate test data
4. `$ sh all.sh`: Submit jobs running on Prince

### Explanation on Building
* See `mkall.sh` and `Makefile` for building details.
* DO NOT use `clang` compiler since it does not support `constexpr __builtin`
  functions. Even if you use `clang`, the compiler will not complain about
  this but only produce an incorrect program since a special workaround is
  applied to `common.h` to by pass my editor's grammar check (which uses
  `clangd` as back-end).
* The running scripts under `jobs/` are generated by `jobs/gen.cpp`. See the
  next section for generating other datasets.
* It is highly likely to go wrong if you simply runs `make` since the
  `fftw-mpi/openmpi/intel` module is not compatible with `openmpi/gnu` module.
* Avoid using `openmpi/gnu/4.0.2` module, since it is highly likely to cause
  the following error and the program will hang when running on multiple nodes.
  ```
  Open MPI detected an inbound MPI TCP connection request from a peer
  that appears to be part of this MPI job (i.e., it identified itself as
  part of this Open MPI job), but it is from an IP address that is
  unexpected.  This is highly unusual.
  ```
  Using `openmpi/gnu/3.1.4` would be fine.
* Avoid using `openmpi/intel` since the `icpc` compiler would complain about
  many errors in standard headers and it seems that `icpc` does not support
  C++17 correctly. (`fftw-mpi.cpp` is composed separately so it could be
  compiled with `icpc`)

### Notes on Generating Datasets
* In this section, I assumes that your working directory is `jobs/`.
* You may generate other running scripts by compiling and running
  `gen.cpp`.
* To compile `gen.cpp`, simply run `g++ -o gen gen.cpp` with a GCC with proper
  C++14 support. (That is `gcc/9.1.0` on Prince)
* To change the length of input/output, modify the array `Ns` defined at line
  22 in `gen.cpp`.
* To change the number of process running on each node and the number of node
  will be used, modify `nTaskPerNode` and `nNode` defined at line 29 and 30 in
  `gen.cpp`. (These two numbers also defines the total number of process)
* To change the amount of memory allocated to your jobs, modify the `#SBATCH
  --mem=???` part at line 58 in `gen.cpp`.
* To change the partition on which the test will run, modify the `#SBATCH
  --partition=???` part at about line 59 in `gen.cpp`.
* If you changed the partition, you will also need to modify `../mkall.sh` and
  rebuild the binaries. See the next section for details.

### Notes on Partitions
* To get comparable results, you may want to run every versions on a same
  partition.
* For different partitions, they may have different processors, the below shows
  the architectures used by some of partitions.
* I tested some partitions for exact processor architecture (See
  `jobs/lscpu/`). The results are shown below.
  + Architecture `Ivy Bridge-EP`: `c28`, `c29`, `c30`, `c31`, `c32_38`,
    `c32_41`, `c39_41`
  + Architecture `Haswell-EP`: `c26`, `c27`
  + Architecture `Broadwell-EP`: `c01_25`, `c01_17`, `c18_25`
  + Architecture `Skylake-SP`: `c42`
  + Architecture `Cascade Lake-SP`: `c43`
* Some of partitions do not have sufficient memory for the number of cores
  specified by the jobs, you need to figure it out on your own to choose a
  proper partition.
* Do not run `fftw-mpi` on partitions with `Ivy Bridge` architecture or it will
  get `SIGILL`. (You cannot avoid this error by changing building options. It
  seems that the pre-built FFTW-MPI library contains those instructions not
  supported by `Ivy Bridge`.)
* By specifying partition, you may want to modify the `export MARCH=???` part
  at about line 15 in `mkall.sh` to tell the compiler the correct target so it
  can do some optimization.
  + For `Ivy Bridge-EP`, modify that line to
    ```
    export MARCH="-march=ivybridge -mtune=ivybridge"
    ```
  + For `Haswell-EP`, modify that line to
    ```
    export MARCH="-march=haswell -mtune=haswell"
    ```
  + For `Broadwell-EP`, modify that line to
    ```
    export MARCH="-march=broadwell -mtune=broadwell"
    ```
  + For `Skylake-SP`, modify that line to
    ```
    export MARCH="-march=skylake-avx512 -mtune=skylake-avx512"
    ```
  + For `Cascade Lake-SP`, modify that line to
    ```
    export MARCH="-march=cascadelake -mtune=cascadelake"
    ```
* I would suggest against modifying line 7 of `mkall.sh` because the `icpc`
  compiler uses very different set of `-march` options and it does not really
  change the test timing since FFTW-MPI library is pre-built.
* The results used in presentation were generated on `c26` except for
  #Process=1, which is generated on `c18_25` since `c26` does not allocate 10GB
  memory for a single process task. Results generated on different partitions
  (especially with different architectures) are not comparable, so I
  regenerated data on `c01_17`.
* Actually the regeneration of results on `c01_17` started on one day before
  the presentation but the jobs were still PENDING even when the class on May 4
  2020 began. So we used the old data got from `c18_25` and `c26` in the
  presentation as you see in commit 53865c9a07e9b8775e66af8bfd6dc68e5a756c87.

## Build and Run on Your Own Computer
* You should be able to build using the conventional `make` command once you
  have sufficiently high version of FFTW-MPI, OpenMPI and GCC installed.
* You can run tests manually by generating data, running FFT and comparing
  outputs using the tools compiled from `gen.cpp`, `cmp.cpp`.

## Source Files
* `mpi-fftw.cpp`: The FFTW-MPI reference implementation.
* `mpi-h.cpp`: The final version of MPI implementation of Bluestein's
  algorithm.
* `gen.cpp`: The data generator, generating a complex number sequence Z with
  given length and saving them to a file. (<i>-100 &le; Re(Z<sub>k</sub>),
  Im(Z<sub>k</sub>) &le; 100</i>)
* `cmp.cpp`: The comparator, calculating the maximum absolute difference of two
  complex number sequences with given length.
* `cat.cpp`: The examiner, printing the complex numbers from a file with given
  count and offset.
* `comp.cpp`: Test if a self made complex number type is faster than
  `std::complex`. (Result: self made complex number is faster)
* `ldexp.cpp`: Test if calling `ldexp()` is faster than simple multiplication
  and division. (Result: `ldexp()` is faster when performing a single
  calculation)
* `sincos.cpp`: Test if calling both `cos()` and `sin()` is faster than
  `exp(std::complex)`. (Result: calling `cos()` and `sin()` is faster)
* `testin.cpp`: Test if C standard I/O is compatible with MPI file I/O.
  (Result: they are compatible)
* `naive.cpp`: Naive implementation of DFT. (Serial)
* `ctdit.cpp`: Decimation in time radix-2 Cooley-Tucky algorithm. (Serial)
* `bluestein*.cpp`: Various implementations of Bluestein's algorithm. (Serial)
  + `bluestein*.cpp`: All twiddling factors are precomputed.
  + `bluestein-log.cpp`: Only twiddling factors for N=power of 2 are
    precomputed. (Result: is the fastest serial implementation)
  + `bluestein-no.cpp`: No precomputed twiddling factors. (Result: no
    observable performance gained compared to `-log.cpp`)
  + `bluestein-red.cpp` (based on `-log.cpp`): Reduce calculations in the
    first level of forward FFT and the last level of backward FFT since the
    length of input/output is only half of the length of FFT. (Result: this is
    much harding to implement using MPI and no observable performance gained)
  + `bluestein-soa.cpp` (based on `-log.cpp`): Change the data layout from
    array of structures to structure of arrays. (Result: no observable
    performance gained)
  + `bluestein-tst.cpp` (based on `-log.cpp`): Save the array `B` and `C` in
    `struct Bluestein` to files, used to verify the correctness of planning
    process in early stage of development of MPI implementation.
* `mpi*.cpp`: Various MPI implementations of Bluestein's algorithm. (Parallel)
  + `mpi.cpp`: MPI version of `bluestein-log.cpp`, using `MPI_Sendrecv` to
    exchange the whole block of data when performing butteflies that requires
    data from other process.
  * `mpi-nb.cpp` (based on `mpi.cpp`): A non-blocking version using
    `MPI_Isendrecv`. (Result: no observable performance gained)
  * `mpi-h.cpp`: Exchange only half block of data compared to `mpi.cpp`, using
    `MPI_Sendrecv`. (Result: is the fastest MPI implementation)
  * `mpi-prog.cpp`: (based on `mpi-nb.cpp`): Split the data need to be
    exchanged into multiple chunks to utilize more overlap. (Result:
    significant performance degradation)
* `common.h`: Provide a self made complex number class and other common
  definitions.
* `Makefile`: Literally the makefile.
* `mkall.sh`: The build script that you should use when building on Prince
  cluster.
* `jobs/gen.cpp`: The job script generator.
* `jobs/gen.sh`: Run this script to generate test data on Prince cluster.
* `jobs/all.sh`: Run this script to submit all test jobs on Prince cluster.
* `jobs/n*.sh`: Separate test scripts.
* `jobs/lscpu/all.sh`: Run this script to get processor information of various
  partitions on Prince cluster.
* `jobs/lscpu/lscpu.sh`: A job script that print host name and processor
  information.
